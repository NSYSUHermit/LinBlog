ggplot(data = result)+
geom_point(mapping = aes(x = ACC,y = AUC,color = rownames(result)),shape=17)
library(ggplot2)
ggplot(data = result)+
geom_point(mapping = aes(x = ACC,y = AUC,color = rownames(result)),size=17)
library(ggplot2)
ggplot(data = result)+
geom_point(mapping = aes(x = ACC,y = AUC,color = rownames(result)),size=7)
result <- data.frame()
result = data.frame(c(0.9994803480263759,0.8678545237199384))
result$Under_Sampling = data.frame(c(0.9995084373222475,0.8901876266312907))
result$SMOTE = data.frame(c(0.9994663033784401,0.9023405417267101))
result$ADASYN = data.frame(c(0.9993890578147933,0.8982438459344533))
result$GAN = data.frame(c(0.999557596696722,0.9003572630796582))
colnames(result) <- c("Non Sampling","Under Sampling","SMOTE","ADASYN","GAN")
rownames(result) <- c("ACC","AUC")
#result$index <- c("ACC","AUC")
result  = data.frame(t(result))
result
library(ggplot2)
ggplot(data = result)+
geom_point(mapping = aes(x = ACC,y = AUC,color = rownames(result)),size=7)
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
![](/post/2019-11-25-gan-based-small-sample-augmentation_files/1.jfif)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::insert_image_addin()
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
# Read data
data_csv <- read.csv("20191202_1471_CRE_46-non-CRE_49_Intensity.csv")
# arrange
if (!require(tidyverse)) install.packages('tidyverse')
library(tidyverse)
#sort data by p.value
data_csv <- arrange(data_csv,p.value)
#transpose data
name_protein <- data_csv[,1]
data <- as.data.frame(t(data_csv))
data <- data[-c(1:3),]
#data name
name_variable <- names(data)
data_name <- data.frame(name_variable,name_protein)
data_name <- as.data.frame(t(data_name))
#label CRE as factor
data$CRE <- as.factor(c(rep(1,46),rep(0,49)))
df1 = data
#classification function
pca.clf <- function(lgr = TRUE,nb = TRUE,knn = TRUE,rf = TRUE,svm = TRUE,pca_num){
#filter pca num
df = as.data.frame(df1[,c(1:pca_num)])
names(df) = names(df1[c(1:pca_num)])
df$CRE = df1$CRE
#ml model training
library(caret)
#svm
if(svm){
if (!require(e1071))install.packages('e1071')
library(e1071)
test.pred <- vector()
for(i in c(1:dim(df)[1])){
train = df[-i, ]
test  = df[i, ]
#pca processing
pca <- prcomp(~.-CRE, data=train)
train_pca <- as.data.frame(pca$x)
train_pca$CRE <- train$CRE
#Matrix multiplication
test_pca = as.data.frame(as.matrix(test[,-dim(test)[2]])%*%as.matrix(pca$rotation))
test_pca$CRE <- test$CRE
svm_model = svm(formula = CRE ~ .,data = train_pca)
test.pred[i] = as.integer(predict(svm_model, test_pca))-1
}
#result present
confus.matrix = table(test.pred,df$CRE)
if(dim(confus.matrix)[1] > 1){
num11 <- confus.matrix[2,2]
num00 <- confus.matrix[1,1]
}else{
num11 = confus.matrix[1,2]
num00 = 0
}
svm <- c("SupportVectorMachine",num11+num00,(num11+num00)/sum(confus.matrix),num11,as.integer(num00),num11/sum(confus.matrix[,2]),num00/sum(confus.matrix[,1]))
}
#rf
if(rf){
if (!require(randomForest)) install.packages('randomForest')
library(randomForest)
test.pred <- vector()
for(i in c(1:dim(df)[1])){
train = df[-i, ]
test  = df[i, ]
#pca processing
pca <- prcomp(~.-CRE, data=train)
train_pca <- as.data.frame(pca$x)
train_pca$CRE <- train$CRE
#Matrix multiplication
test_pca = as.data.frame(as.matrix(test[,-dim(test)[2]])%*%as.matrix(pca$rotation))
test_pca$CRE <- test$CRE
rf_model = randomForest(CRE~.,data=train_pca,ntree=150# num of decision Tree
)
test.pred[i] = as.integer(predict(rf_model, test_pca))-1
}
#result present
confus.matrix = table(test.pred,df$CRE)
if(dim(confus.matrix)[1] > 1){
num11 <- confus.matrix[2,2]
num00 <- confus.matrix[1,1]
}else{
num11 = confus.matrix[1,2]
num00 = 0
}
rf <- c("RandomForest",num11+num00,(num11+num00)/sum(confus.matrix),num11,as.integer(num00),num11/sum(confus.matrix[,2]),num00/sum(confus.matrix[,1]))
}
# knn
if(knn){
if (!require(class))install.packages("class")
library(class)
test.pred <- vector()
for(i in c(1:dim(df)[1])){
train = df[-i, ]
test  = df[i, ]
#pca processing
pca <- prcomp(~.-CRE, data=train)
train_pca <- as.data.frame(pca$x)
train_pca$CRE <- train$CRE
#Matrix multiplication
test_pca = as.data.frame(as.matrix(test[,-dim(test)[2]])%*%as.matrix(pca$rotation))
test_pca$CRE <- test$CRE
#pred
test.pred[i] <- knn(train = train_pca[,-length(train_pca)], test = test_pca[,-length(test_pca)], cl = train_pca[,length(train_pca)], k = 5)   # knn distance = 5
}
#result present
confus.matrix = table(test.pred,df$CRE)
if(dim(confus.matrix)[1] > 1){
num11 <- confus.matrix[2,2]
num00 <- confus.matrix[1,1]
}else{
num11 = confus.matrix[1,2]
num00 = 0
}
knn <- c("NearestNeighbors",num11+num00,(num11+num00)/sum(confus.matrix),num11,as.integer(num00),num11/sum(confus.matrix[,2]),num00/sum(confus.matrix[,1]))
}
# nb
if(nb){
test.pred <- vector()
for(i in c(1:dim(df)[1])){
train = df[-i, ]
test  = df[i, ]
#pca processing
pca <- prcomp(~.-CRE, data=train)
train_pca <- as.data.frame(pca$x)
train_pca$CRE <- train$CRE
#Matrix multiplication
test_pca = as.data.frame(as.matrix(test[,-dim(test)[2]])%*%as.matrix(pca$rotation))
test_pca$CRE <- test$CRE
#pred
nb_model=naiveBayes(CRE~., data=train_pca)
test.pred[i] = as.integer(predict(nb_model, test_pca))-1
}
#result present
confus.matrix = table(test.pred,df$CRE)
if(dim(confus.matrix)[1] > 1){
num11 <- confus.matrix[2,2]
num00 <- confus.matrix[1,1]
}else{
num11 = confus.matrix[1,2]
num00 = 0
}
nb <- c("NavieBayes",num11+num00,(num11+num00)/sum(confus.matrix),num11,as.integer(num00),num11/sum(confus.matrix[,2]),num00/sum(confus.matrix[,1]))
}
#lgr
if(lgr){
test.pred <- vector()
df$CRE = as.numeric(df$CRE)-1
#pca processing
pca <- prcomp(~.-CRE, data=train)
train_pca <- as.data.frame(pca$x)
train_pca$CRE <- train$CRE
#Matrix multiplication
test_pca = as.data.frame(as.matrix(test[,-dim(test)[2]])%*%as.matrix(pca$rotation))
test_pca$CRE <- test$CRE
#pred
for(i in c(1:dim(df)[1])){
train = df[-i, ]
test  = df[i, ]
lr_model<-glm(formula=CRE~.,data=train,family=binomial)
test.pred[i] <- ifelse(predict(lr_model, test,type = "response") > 0.5, 1, 0)
}
#result present
confus.matrix = table(test.pred,df$CRE)
if(dim(confus.matrix)[1] > 1){
num11 <- confus.matrix[2,2]
num00 <- confus.matrix[1,1]
}else{
num11 = confus.matrix[1,2]
num00 = 0
}
lgr <- c("LogisticRegression",num11+num00,(num11+num00)/sum(confus.matrix),num11,as.integer(num00),num11/sum(confus.matrix[,2]),num00/sum(confus.matrix[,1]))
}
#return results
result <- c(46,49,pca_num,lgr,nb,knn,rf,svm)
return(result)
}
a = as.data.frame(t(pca.clf(pca=1)))
for(i in c(2:25)){
b = as.data.frame(t(pca.clf(pca=i)))
a = rbind(a,b)
}
View(a)
names(a) = c("num_cre","num_non","num_pca","method","right_pred_num","acc","right_pred_cre_num","right_pred_ncre_num","right_pred_cre_acc","right_pred_ncre_acc","method","right_pred_num","acc","right_pred_cre_num","right_pred_ncre_num","right_pred_cre_acc","right_pred_ncre_acc","method","right_pred_num","acc","right_pred_cre_num","right_pred_ncre_num","right_pred_cre_acc","right_pred_ncre_acc","method","right_pred_num","acc","right_pred_cre_num","right_pred_ncre_num","right_pred_cre_acc","right_pred_ncre_acc","method","right_pred_num","acc","right_pred_cre_num","right_pred_ncre_num","right_pred_cre_acc","right_pred_ncre_acc")
write.csv(a,file = "pca_result.csv",row.names = FALSE)
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
---
title: SQL Query Optimization
author: Hermit
date: '2020-01-06'
slug: sql-query-optimization
categories:
- SQL
tags:
- small-talk
---
這學期修的資管系資料庫系統期末報告，主要是建立一個有著大資料表的資料庫系統，並且針對系統下三個查詢語句，並且進行查詢句的優化，
blogdown:::insert_image_addin()
SELECT t1.code , t1.name
blogdown:::serve_site()
blogdown:::serve_site()
我們第三個查詢句為：
```{r eval = FALSE}
SELECT t1.code , t1.name
from (
(SELECT condition_table.CODE,condition_table.NAME ,condition_table."indicator" , condition_table.date1
FROM condition_table
WHERE condition_table.date1 = 20191108 AND condition_table.COND_MATCH = 1) t1
join
(SELECT CODE
FROM stock_price_data
WHERE dirction = 'increasing' AND date1 >= 20191106 AND date1 <= 20191108
HAVING COUNT(dirction)=3
GROUP BY CODE,NAME) t2
ON t1.code = t2.code
)
```
主要目的是為了查詢連日上漲且符合進場標準的股票，列出名稱、代碼、以及進場條件，在這邊我們設定是三天內皆上漲，且現在正好是滿足進場條件的股票，因此這個查詢句需要join兩個表格：股價資料以及進場條件資料
我們第三個查詢句為：
```{r eval = FALSE}
SELECT t1.code , t1.name
from (
(SELECT condition_table.CODE,condition_table.NAME ,condition_table."indicator" , condition_table.date1
FROM condition_table
WHERE condition_table.date1 = 20191108 AND condition_table.COND_MATCH = 1) t1
join
(SELECT CODE
FROM stock_price_data
WHERE dirction = 'increasing' AND date1 >= 20191106 AND date1 <= 20191108
HAVING COUNT(dirction)=3
GROUP BY CODE,NAME) t2
ON t1.code = t2.code
)
```
主要目的是為了查詢連日上漲且符合進場標準的股票，列出名稱、代碼、以及進場條件，在這邊我們設定是三天內皆上漲，且現在正好是滿足進場條件的股票，因此這個查詢句需要join兩個表格：股價資料以及進場條件資料
?
= =
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
而目前wgan的效果未必比較好，雖然wgan的設置可以讓兩個神經網路的loss有同時下降的可能
而目前wgan的效果未必比較好，雖然wgan的設置可以讓兩個神經網路的loss有同時下降的可能，但從我們的結果論來看並不一定會有比較好的效果，也可能是wgan設定的Weight Clipping沒有比較好的定義
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
link = paste0("https://www.cnyes.com/twstock/ps_historyprice/2887.htm")
price <- read_html(link)
#get the table
price_data1 <- html_nodes(price,"div.mbx.bd3")
price_data <- html_text(price_data1)
View(price_data1)
price_data
#get the table
price_data1 <- html_nodes(price,"div.tab")
price_data <- html_text(price_data1)
price_data
blogdown:::new_post_addin()
blogdown:::serve_site()
---
title: 'OCGAN Pratice: CRE Bateria data'
author: Hermit
date: '2020-02-04'
slug: ocgan-pratice-cre-bateria-data
categories:
- gan
- deep-learning
- Python
tags:
- classification
- neural network
---
這次使用之前分析過的CRE資料，來嘗試使用OCGAN，但因原先資料CRE:NON比數為46:49，為了達到不平衡的效果，因此最後採用16:49的比例，從46個CRE中取隨機16個，
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
library(raster)
library(raster)
feature_csv <- read.csv("C:\\Users\\User\\OneDrive - student.nsysu.edu.tw\\Documents\\Python\\GAN\\feature.csv")
#best = c("V993","V322","V864","V689","V598","V1156","V240","V395","V1255","V1218","V634","V529", "V869", "V410", "V521", "V32", "V1201", "V478", "V306", "V964", "V1122", "V485", "V690", "V947", "V677", "V1444", "V832", "V1", "V517", "V351", "V9", "V109", "V872", "V518", "V1239", "V270", "V695", "V147", "V524", "V679", "V320", "V356", "V232", "V687", "V112", "V983", "V146", "V345", "V520", "V198", "V59", "V408", "V110", "V250", "V1275", "V60", "V1253", "V459", "V522", "V889", "V403", "V269", "V87", "V530", "V839", "V399", "V861", "V242", "V823", "V58", "V627", "V84", "V321", "V50", "V483", "V475", "V1396", "V1411", "V1285", "V1093", "V1378", "V413", "V525", "V671", "V30", "V95", "V1199", "V767", "V809", "V1404", "V1401", "V113", "V1198", "V1405", "V1398", "V1209", "V1407", "V1352", "V271", "V528", "V805", "V1397", "V753", "V200", "V1400", "V1408", "V1394", "V593", "V1157", "V233", "V268", "V576", "V181", "V1395", "V820", "V1257", "V514", "V669", "V943", "V489", "V937", "V486", "V513", "V1143", "V966", "V980", "V1274", "V1403", "V343", "V686", "V653", "V1281", "V234", "V1279", "V523", "V870", "V959", "V1278", "V871", "V5", "V775", "V845", "V1211", "V1110", "V1273", "V995", "V1276", "V873", "V595", "V1280", "V1034", "V1228", "V1012", "V1226", "V1094", "V511", "V944", "V1068", "V1146", "V313", "V821", "V122", "V1227", "V386", "V771", "V551", "V538", "V1220", "V1179")
num = 1
best = as.character(feature_csv[1:(61 - length(which(feature_csv[,num] == ""))),num])
bestdf = data[,best]
# Read data
data_csv <- read.csv("20191202_1471_CRE_46-non-CRE_49_Intensity.csv")
# Read data
data_csv <- read.csv("C:\\Users\\User\\OneDrive - student.nsysu.edu.tw\\Educations\\NSYSU\\fu_chung\\bacterial - PCA\\20191202_1471_CRE_46-non-CRE_49_Intensity.csv")
# arrange
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
#sort data by p.value
data_csv <- arrange(data_csv,p.value)
#transpose data
name_protein <- data_csv[,1]
data <- as.data.frame(t(data_csv))
data <- data[-c(1:3),]
#data name
name_variable <- names(data)
data_name <- data.frame(name_variable,name_protein)
data_name <- as.data.frame(t(data_name))
library(raster)
feature_csv <- read.csv("C:\\Users\\User\\OneDrive - student.nsysu.edu.tw\\Documents\\Python\\GAN\\feature.csv")
#best = c("V993","V322","V864","V689","V598","V1156","V240","V395","V1255","V1218","V634","V529", "V869", "V410", "V521", "V32", "V1201", "V478", "V306", "V964", "V1122", "V485", "V690", "V947", "V677", "V1444", "V832", "V1", "V517", "V351", "V9", "V109", "V872", "V518", "V1239", "V270", "V695", "V147", "V524", "V679", "V320", "V356", "V232", "V687", "V112", "V983", "V146", "V345", "V520", "V198", "V59", "V408", "V110", "V250", "V1275", "V60", "V1253", "V459", "V522", "V889", "V403", "V269", "V87", "V530", "V839", "V399", "V861", "V242", "V823", "V58", "V627", "V84", "V321", "V50", "V483", "V475", "V1396", "V1411", "V1285", "V1093", "V1378", "V413", "V525", "V671", "V30", "V95", "V1199", "V767", "V809", "V1404", "V1401", "V113", "V1198", "V1405", "V1398", "V1209", "V1407", "V1352", "V271", "V528", "V805", "V1397", "V753", "V200", "V1400", "V1408", "V1394", "V593", "V1157", "V233", "V268", "V576", "V181", "V1395", "V820", "V1257", "V514", "V669", "V943", "V489", "V937", "V486", "V513", "V1143", "V966", "V980", "V1274", "V1403", "V343", "V686", "V653", "V1281", "V234", "V1279", "V523", "V870", "V959", "V1278", "V871", "V5", "V775", "V845", "V1211", "V1110", "V1273", "V995", "V1276", "V873", "V595", "V1280", "V1034", "V1228", "V1012", "V1226", "V1094", "V511", "V944", "V1068", "V1146", "V313", "V821", "V122", "V1227", "V386", "V771", "V551", "V538", "V1220", "V1179")
num = 1
best = as.character(feature_csv[1:(61 - length(which(feature_csv[,num] == ""))),num])
bestdf = data[,best]
bestdf$CRE <- as.factor(c(rep(1,46),rep(0,49)))
rf_model = randomForest(CRE~.,
data=bestdf,
ntree=500        # num of decision Tree
)
library(randomForest)
rf_model = randomForest(CRE~.,
data=bestdf,
ntree=500        # num of decision Tree
)
plot(rf_model)
which.min(rf_model$err.rate[,1])
A = as.data.frame(rf_model$importance)
A$names <- row.names(A)
A = A[order(A$MeanDecreaseGini,decreasing = T),]
impo <- A[1:50,2]
impodf = data[,impo]
impodf = data[,impo]
A
library(randomForest)
rf_model = randomForest(CRE~.,
data=bestdf,
ntree=500        # num of decision Tree
)
plot(rf_model)
which.min(rf_model$err.rate[,1])
A = as.data.frame(rf_model$importance)
A$names <- row.names(A)
A = A[order(A$MeanDecreaseGini,decreasing = T),]
impo <- A[1:50,2]
A
impodf = data[,impo]
data[,impo]
View(data)
data[,impo]
impo
rf_model$importance
library(randomForest)
rf_model = randomForest(CRE~.,
data=data,
ntree=500        # num of decision Tree
)
View(data)
data$CRE <- as.factor(c(rep(1,46),rep(0,49)))
View(data)
library(randomForest)
rf_model = randomForest(CRE~.,
data=data,
ntree=500        # num of decision Tree
)
plot(rf_model)
which.min(rf_model$err.rate[,1])
A = as.data.frame(rf_model$importance)
A$names <- row.names(A)
A = A[order(A$MeanDecreaseGini,decreasing = T),]
impo <- A[1:50,2]
A
impodf = data[,impo]
impodf$CRE <- as.factor(c(rep(1,46),rep(0,49)))
View(A)
head(A)
library(randomForest)
rf_model = randomForest(CRE~.,
data=data,
ntree=500        # num of decision Tree
)
plot(rf_model)
which.min(rf_model$err.rate[,1])
A = as.data.frame(rf_model$importance)
A$names <- row.names(A)
A = A[order(A$MeanDecreaseGini,decreasing = T),]
impo <- A[1:50,2]
A[c(1:50),]
impodf = data[,impo]
impodf$CRE <- as.factor(c(rep(1,46),rep(0,49)))
result <- read.csv("C:\\Users\\User\\OneDrive - student.nsysu.edu.tw\\Educations\\NSYSU\\fu_chung\\bacterial - PCA\\result.csv")
result
label <- read.csv("C:\\Users\\User\\OneDrive - student.nsysu.edu.tw\\Educations\\NSYSU\\fu_chung\\bacterial - PCA\\label.csv")
label
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::insert_image_addin()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
servr::daemon_stop(3)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::update_meta_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
---
title: Compare to OCGAN & SMOTE & ADASYN in CRE data Simulation
author: Hermit
date: '2020-02-25'
slug: compare-to-ocgan-smote-adasyn-in-cre-data-simulation
categories:
- gan
- machine-learning
- deep-learning
- Python
tags:
- classification
- neural network
---
與上禮拜那篇文章一樣，只是這次將資料改為CRE data，希望也有良好的表現
# 1.  讀入資料
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
log(2)
log(1)
log(0.5)
blogdown:::serve_site()
blogdown:::serve_site()
