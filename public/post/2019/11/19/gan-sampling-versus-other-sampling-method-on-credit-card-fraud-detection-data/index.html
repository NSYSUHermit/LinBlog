<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>GAN Sampling Versus Other Sampling Method On Credit Card Fraud Detection Data | Lin&#39;s Blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
&Lfr;&Ifr;&Nfr;'&Sfr; &Bfr;&Lfr;&Ofr;&Gfr;
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
        <span class="date">2019-11-19</span>
        
        
        
          
        
        
        
        <span><a href="https://github.com/yihui/hugo-xmag/edit/master/exampleSite/content/post/2019-11-19-gan-sampling-versus-other-sampling-method-on-credit-card-fraud-detection-data.Rmd">Edit this page &rarr;</a></span>
        
        
      
      </div>
    </nav>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/deep-learning">deep-learning</a>
  
     &hercon; <a href="/categories/python">Python</a>
  
     &hercon; <a href="/categories/gan">gan</a>
  
     &hercon; <a href="/categories/machine-learning">machine-learning</a>
  
  </div>

  <h1><span class="title">GAN Sampling Versus Other Sampling Method On Credit Card Fraud Detection Data</span></h1>

  
  <h3 class="author">Hermit
</h3>
  

  
  <p>Tags: <a href="/tags/classification">classification</a>; <a href="/tags/neural-network">neural network</a>
  </p>
  
  

</div>



<main>



<p>這次，我將使用一個來自kaggle的不平衡數據資料(link:<a href="https://www.kaggle.com/mlg-ulb/creditcardfraud/version/1" class="uri">https://www.kaggle.com/mlg-ulb/creditcardfraud/version/1</a>).<br />
<img src="/post/2019-11-19-gan-sampling-versus-other-sampling-method-on-credit-card-fraud-detection-data_files/1.jpg" /><br />
該數據集包含了歐洲持卡人2013年9月通過信用卡進行的交易。這些交易發生在兩天之內，在這裡我們有492筆詐騙資料以及284807正常交易資料。該數據集是非常不平衡的，其中陰性樣本（詐欺）佔所有交易的0.172％。它的變量包含數值輸入變量後PCA變換的結果。不幸的是，由於保密問題我們不能得到原始數據的更多背景信息。特徵V1，V2，…… V28與PCA獲得的主成分，還沒有被轉化與PCA的變量是“交易時間”和“交易金額”。特徵“時間”為與第一筆交易所間隔的時間（秒）。特徵“金額”是交易金額，該功能可以被用於例如依賴性成本靈敏學習。特徵“class”是反應變量，並將其在欺詐的情況下取值為1。</p>
<p>而這次我的主要目的是，用數種常見的採樣方法與gan的過採樣方法進行簡單的比較，以平衡後的數據對其做相同的分類機器學習，這裡一慮採用random forest(n = 100)來建立分類器，屆時比較各採樣方法的accuracy與auc。</p>
<div id="data-visualization" class="section level1">
<h1>Data Visualization</h1>
<p>在這裡我會做一些資料的切割以及簡單的視覺化，提供一些資料的簡單資訊。</p>
<pre class="python"><code>import pandas as pd
import numpy as np

df = pd.read_csv(&#39;C:/Users/User/OneDrive - student.nsysu.edu.tw/Documents/dataset/creditcard.csv&#39;)
fraud = df[df[&#39;Class&#39;].isin([1])]
normal = df[df[&#39;Class&#39;].isin([0])]
df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Time
</th>
<th>
V1
</th>
<th>
V2
</th>
<th>
V3
</th>
<th>
V4
</th>
<th>
V5
</th>
<th>
V6
</th>
<th>
V7
</th>
<th>
V8
</th>
<th>
V9
</th>
<th>
…
</th>
<th>
V21
</th>
<th>
V22
</th>
<th>
V23
</th>
<th>
V24
</th>
<th>
V25
</th>
<th>
V26
</th>
<th>
V27
</th>
<th>
V28
</th>
<th>
Amount
</th>
<th>
Class
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.0
</td>
<td>
-1.359807
</td>
<td>
-0.072781
</td>
<td>
2.536347
</td>
<td>
1.378155
</td>
<td>
-0.338321
</td>
<td>
0.462388
</td>
<td>
0.239599
</td>
<td>
0.098698
</td>
<td>
0.363787
</td>
<td>
…
</td>
<td>
-0.018307
</td>
<td>
0.277838
</td>
<td>
-0.110474
</td>
<td>
0.066928
</td>
<td>
0.128539
</td>
<td>
-0.189115
</td>
<td>
0.133558
</td>
<td>
-0.021053
</td>
<td>
149.62
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0.0
</td>
<td>
1.191857
</td>
<td>
0.266151
</td>
<td>
0.166480
</td>
<td>
0.448154
</td>
<td>
0.060018
</td>
<td>
-0.082361
</td>
<td>
-0.078803
</td>
<td>
0.085102
</td>
<td>
-0.255425
</td>
<td>
…
</td>
<td>
-0.225775
</td>
<td>
-0.638672
</td>
<td>
0.101288
</td>
<td>
-0.339846
</td>
<td>
0.167170
</td>
<td>
0.125895
</td>
<td>
-0.008983
</td>
<td>
0.014724
</td>
<td>
2.69
</td>
<td>
0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1.0
</td>
<td>
-1.358354
</td>
<td>
-1.340163
</td>
<td>
1.773209
</td>
<td>
0.379780
</td>
<td>
-0.503198
</td>
<td>
1.800499
</td>
<td>
0.791461
</td>
<td>
0.247676
</td>
<td>
-1.514654
</td>
<td>
…
</td>
<td>
0.247998
</td>
<td>
0.771679
</td>
<td>
0.909412
</td>
<td>
-0.689281
</td>
<td>
-0.327642
</td>
<td>
-0.139097
</td>
<td>
-0.055353
</td>
<td>
-0.059752
</td>
<td>
378.66
</td>
<td>
0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1.0
</td>
<td>
-0.966272
</td>
<td>
-0.185226
</td>
<td>
1.792993
</td>
<td>
-0.863291
</td>
<td>
-0.010309
</td>
<td>
1.247203
</td>
<td>
0.237609
</td>
<td>
0.377436
</td>
<td>
-1.387024
</td>
<td>
…
</td>
<td>
-0.108300
</td>
<td>
0.005274
</td>
<td>
-0.190321
</td>
<td>
-1.175575
</td>
<td>
0.647376
</td>
<td>
-0.221929
</td>
<td>
0.062723
</td>
<td>
0.061458
</td>
<td>
123.50
</td>
<td>
0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
2.0
</td>
<td>
-1.158233
</td>
<td>
0.877737
</td>
<td>
1.548718
</td>
<td>
0.403034
</td>
<td>
-0.407193
</td>
<td>
0.095921
</td>
<td>
0.592941
</td>
<td>
-0.270533
</td>
<td>
0.817739
</td>
<td>
…
</td>
<td>
-0.009431
</td>
<td>
0.798278
</td>
<td>
-0.137458
</td>
<td>
0.141267
</td>
<td>
-0.206010
</td>
<td>
0.502292
</td>
<td>
0.219422
</td>
<td>
0.215153
</td>
<td>
69.99
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
<p>
5 rows × 31 columns
</p>
</div>
<pre class="python"><code>%matplotlib inline
by_fraud = df.groupby(&#39;Class&#39;)
by_fraud.size().plot(kind = &#39;bar&#39;)</code></pre>
<p>&lt;matplotlib.axes._subplots.AxesSubplot at 0x227fb41c940&gt;</p>
<p><img src="/post/2019-11-19-gan-sampling-versus-other-sampling-method-on-credit-card-fraud-detection-data_files/output_2_1.png" /></p>
<pre class="python"><code>fraud[&#39;Amount&#39;].describe()</code></pre>
<p>count 492.000000<br />
mean 122.211321<br />
std 256.683288<br />
min 0.000000<br />
25% 1.000000<br />
50% 9.250000<br />
75% 105.890000<br />
max 2125.870000<br />
Name: Amount, dtype: float64</p>
<pre class="python"><code>normal[&#39;Amount&#39;].describe()</code></pre>
<p>count 284315.000000<br />
mean 88.291022<br />
std 250.105092<br />
min 0.000000<br />
25% 5.650000<br />
50% 22.000000<br />
75% 77.050000<br />
max 25691.160000<br />
Name: Amount, dtype: float64</p>
<p>上面是將異常與正常的資料分開做簡單統計量呈現，我們可看出正常資料為284315筆、異常資料為492筆，大約0.9983:0.0017，此為不平衡資料，因此在這裡我們嘗試比較數個under sampling、oversampling的幾個採樣方法，並在採樣後皆使用隨機森林n=100的方法下去建模，藉此比較各種採樣方法的AIC與Accuracy。我們將使用Non sampling、under sampling、smote以及GAN with one-class classifier。而我皆保留50%原始資料當作validation set，剩餘50%則做為training set。</p>
</div>
<div id="non-sampling" class="section level1">
<h1>Non Sampling</h1>
<p>這裡我將先以不平衡的方式直接建模，以當作其餘資料的對照組。</p>
<pre class="python"><code>from sklearn.model_selection import train_test_split
fraud = df[df[&#39;Class&#39;].isin([1])]
normal = df[df[&#39;Class&#39;].isin([0])]

test_nor, train_nor = train_test_split(normal, test_size = 0.5)
train_fra, test_fra = train_test_split(fraud, test_size = 0.5)
data_train = pd.concat([train_nor,train_fra], axis=0)
data_test = pd.concat([test_nor,test_fra], axis=0) 

train_X = data_train.iloc[:,0:30]
test_X = data_test.iloc[:,0:30]
train_y = data_train[&quot;Class&quot;]
test_y = data_test[&quot;Class&quot;]</code></pre>
<pre class="python"><code>forest = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = forest.fit(train_X, train_y)

test_y_predicted = forest.predict(test_X)
accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)</code></pre>
<p>ACC:0.9994803480263759<br />
AUC:0.8678545237199384</p>
</div>
<div id="under-sampling" class="section level1">
<h1>Under Sampling</h1>
<p>這個地方我採用下採樣方法，即從14w筆正常的訓練資料中抽出246筆與246筆異常資料的492筆資料建立分類模型。</p>
<pre class="python"><code>from sklearn.model_selection import train_test_split
fraud = df[df[&#39;Class&#39;].isin([1])]
normal = df[df[&#39;Class&#39;].isin([0])]

test_nor, train_nor = train_test_split(normal, test_size = 0.0008652375)
train_fra, test_fra = train_test_split(fraud, test_size = 0.5)
data_train = pd.concat([train_nor,train_fra], axis=0)
data_test = pd.concat([test_nor,test_fra], axis=0) </code></pre>
<pre class="python"><code>train_X = data_train.iloc[:,0:30]
test_X = data_test.iloc[:,0:30]
train_y = data_train[&quot;Class&quot;]
test_y = data_test[&quot;Class&quot;]</code></pre>
<pre class="python"><code>from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn import metrics

forest = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = forest.fit(train_X, train_y)

test_y_predicted = forest.predict(test_X)


accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)</code></pre>
<p>ACC:0.9995084373222475<br />
AUC:0.8901876266312907</p>
</div>
<div id="smote" class="section level1">
<h1>SMOTE</h1>
<p>SMOTE是很常見的採樣方法，在這裡我將使用其演算法生成用以平衡的偽資料，以進行後續分類器的訓練。</p>
<pre class="python"><code>from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(train_X, train_y)
X_resampled = pd.DataFrame(X_resampled)
y_resampled = pd.DataFrame(y_resampled)</code></pre>
<pre class="python"><code>X_resampled.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
…
</th>
<th>
20
</th>
<th>
21
</th>
<th>
22
</th>
<th>
23
</th>
<th>
24
</th>
<th>
25
</th>
<th>
26
</th>
<th>
27
</th>
<th>
28
</th>
<th>
29
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
142158.0
</td>
<td>
2.098232
</td>
<td>
-0.120009
</td>
<td>
-1.556132
</td>
<td>
0.138363
</td>
<td>
0.474125
</td>
<td>
-0.357738
</td>
<td>
0.133597
</td>
<td>
-0.197507
</td>
<td>
0.570249
</td>
<td>
…
</td>
<td>
-0.137225
</td>
<td>
-0.335204
</td>
<td>
-0.897051
</td>
<td>
0.270364
</td>
<td>
0.032881
</td>
<td>
-0.182849
</td>
<td>
0.206514
</td>
<td>
-0.079001
</td>
<td>
-0.057921
</td>
<td>
15.88
</td>
</tr>
<tr>
<th>
1
</th>
<td>
120533.0
</td>
<td>
-0.231309
</td>
<td>
-0.100840
</td>
<td>
0.901389
</td>
<td>
0.102371
</td>
<td>
0.193673
</td>
<td>
1.020901
</td>
<td>
0.086328
</td>
<td>
0.467544
</td>
<td>
0.523512
</td>
<td>
…
</td>
<td>
0.021338
</td>
<td>
0.160314
</td>
<td>
0.226446
</td>
<td>
0.136961
</td>
<td>
-1.550618
</td>
<td>
-0.454142
</td>
<td>
-0.752112
</td>
<td>
0.097091
</td>
<td>
0.058509
</td>
<td>
112.78
</td>
</tr>
<tr>
<th>
2
</th>
<td>
143598.0
</td>
<td>
1.707347
</td>
<td>
-2.010102
</td>
<td>
-0.119053
</td>
<td>
-0.621244
</td>
<td>
-1.470343
</td>
<td>
1.004779
</td>
<td>
-1.539303
</td>
<td>
0.351543
</td>
<td>
0.807437
</td>
<td>
…
</td>
<td>
0.417312
</td>
<td>
0.265302
</td>
<td>
0.609864
</td>
<td>
0.017976
</td>
<td>
0.337693
</td>
<td>
-0.339032
</td>
<td>
-0.226504
</td>
<td>
0.021035
</td>
<td>
-0.009205
</td>
<td>
205.63
</td>
</tr>
<tr>
<th>
3
</th>
<td>
77785.0
</td>
<td>
-0.432809
</td>
<td>
0.441564
</td>
<td>
2.135267
</td>
<td>
1.571277
</td>
<td>
0.007931
</td>
<td>
1.057945
</td>
<td>
-0.090015
</td>
<td>
0.405717
</td>
<td>
0.597818
</td>
<td>
…
</td>
<td>
0.010397
</td>
<td>
-0.325426
</td>
<td>
-0.474865
</td>
<td>
-0.051495
</td>
<td>
-0.426681
</td>
<td>
-0.322176
</td>
<td>
-0.406189
</td>
<td>
0.233169
</td>
<td>
0.152133
</td>
<td>
7.60
</td>
</tr>
<tr>
<th>
4
</th>
<td>
37257.0
</td>
<td>
1.206999
</td>
<td>
-0.933973
</td>
<td>
0.439600
</td>
<td>
-0.833985
</td>
<td>
-1.045920
</td>
<td>
-0.002520
</td>
<td>
-0.895569
</td>
<td>
0.236254
</td>
<td>
-0.653197
</td>
<td>
…
</td>
<td>
0.079357
</td>
<td>
0.067089
</td>
<td>
-0.156065
</td>
<td>
0.041761
</td>
<td>
-0.358237
</td>
<td>
0.097000
</td>
<td>
-0.379807
</td>
<td>
0.005337
</td>
<td>
0.015024
</td>
<td>
75.00
</td>
</tr>
</tbody>
</table>
<p>
5 rows × 30 columns
</p>
</div>
<pre class="python"><code>forest = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = forest.fit(X_resampled, y_resampled)

test_y_predicted = forest.predict(test_X)
accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)</code></pre>
<p>C:3-gpu-packages_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().</p>
<p>ACC = 0.9994663033784401<br />
AUC = 0.9023405417267101</p>
</div>
<div id="adasyn" class="section level1">
<h1>ADASYN</h1>
<p>ADASYN是另一種過採樣方法，它是Smote的改進版本。它的功能與SMOTE相同，只是稍有改進。創建這些假樣本後，它會向這些點添加一個隨機的小值，從而使其更加真實。換句話說，不是所有樣本都與原始樣本線性相關，而是它們具有更多的變異(variance)，即它們是零散的。</p>
<pre class="python"><code>from imblearn.over_sampling import ADASYN
X_resampled, y_resampled = ADASYN().fit_resample(train_X, train_y)
X_resampled = pd.DataFrame(X_resampled)
y_resampled = pd.DataFrame(y_resampled)</code></pre>
<pre class="python"><code>forest = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = forest.fit(X_resampled, y_resampled)

test_y_predicted = forest.predict(test_X)
accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(&quot;accuracy = %s&quot;%accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (&quot;test_auc = %d&quot;%test_auc)</code></pre>
<p>C:3-gpu-packages_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().</p>
<p>accuracy = 0.9993890578147933<br />
AUC = 0.8982438459344533</p>
</div>
<div id="gan-with-one-class-svm" class="section level1">
<h1>GAN with One Class SVM</h1>
<div id="the-generator" class="section level2">
<h2>The generator</h2>
<p>Set up modules</p>
<pre class="python"><code>%matplotlib inline
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm_notebook as tqdm
from keras.models import Model
from keras.layers import Input, Reshape
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import UpSampling1D, Conv1D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, SGD
from keras.callbacks import TensorBoard
from sklearn.preprocessing import StandardScaler

dim = 30
num = 246
g_data = f_x</code></pre>
<p>Using TensorFlow backend.</p>
<p>Standard Scaler</p>
<pre class="python"><code>ss = StandardScaler()
g_data = pd.DataFrame(ss.fit_transform(g_data))</code></pre>
<pre class="python"><code>def get_generative(G_in, dense_dim=200, out_dim= dim, lr=1e-3):
    x = Dense(dense_dim)(G_in)
    x = Activation(&#39;tanh&#39;)(x)
    G_out = Dense(out_dim, activation=&#39;tanh&#39;)(x)
    G = Model(G_in, G_out)
    opt = SGD(lr=lr)
    G.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt)
    return G, G_out

G_in = Input(shape=[10])
G, G_out = get_generative(G_in)
G.summary()</code></pre>
<p>WARNING:tensorflow:From C:3-gpu-packages_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages<em>impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</em>________________________________________________________________
Layer (type) Output Shape Param #<br />
=================================================================
input_1 (InputLayer) (None, 10) 0<br />
_________________________________________________________________
dense_1 (Dense) (None, 200) 2200<br />
_________________________________________________________________
activation_1 (Activation) (None, 200) 0<br />
_________________________________________________________________
dense_2 (Dense) (None, 30) 6030<br />
=================================================================
Total params: 8,230
Trainable params: 8,230
Non-trainable params: 0
_________________________________________________________________</p>
</div>
<div id="section" class="section level2">
<h2>建立判別器</h2>
<pre class="python"><code>def get_discriminative(D_in, lr=1e-3, drate=.25, n_channels= dim, conv_sz=5, leak=.2):
    x = Reshape((-1, 1))(D_in)
    x = Conv1D(n_channels, conv_sz, activation=&#39;relu&#39;)(x)
    x = Dropout(drate)(x)
    x = Flatten()(x)
    x = Dense(n_channels)(x)
    D_out = Dense(2, activation=&#39;sigmoid&#39;)(x)
    D = Model(D_in, D_out)
    dopt = Adam(lr=lr)
    D.compile(loss=&#39;binary_crossentropy&#39;, optimizer=dopt)
    return D, D_out

D_in = Input(shape=[dim])
D, D_out = get_discriminative(D_in)
D.summary()</code></pre>
<p>WARNING:tensorflow:From C:3-gpu-packages_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.</p>
<p>WARNING:tensorflow:From C:3-gpu-packages<em>backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use <code>rate</code> instead of <code>keep_prob</code>. Rate should be set to <code>rate = 1 - keep_prob</code>.
</em>________________________________________________________________
Layer (type) Output Shape Param #<br />
=================================================================
input_2 (InputLayer) (None, 30) 0<br />
_________________________________________________________________
reshape_1 (Reshape) (None, 30, 1) 0<br />
_________________________________________________________________
conv1d_1 (Conv1D) (None, 26, 30) 180<br />
_________________________________________________________________
dropout_1 (Dropout) (None, 26, 30) 0<br />
_________________________________________________________________
flatten_1 (Flatten) (None, 780) 0<br />
_________________________________________________________________
dense_3 (Dense) (None, 30) 23430<br />
_________________________________________________________________
dense_4 (Dense) (None, 2) 62<br />
=================================================================
Total params: 23,672
Trainable params: 23,672
Non-trainable params: 0
_________________________________________________________________</p>
</div>
<div id="section-1" class="section level2">
<h2>串接兩神經網路</h2>
<pre class="python"><code>def set_trainability(model, trainable=False):
    model.trainable = trainable
    for layer in model.layers:
        layer.trainable = trainable
        
def make_gan(GAN_in, G, D):
    set_trainability(D, False)
    x = G(GAN_in)
    GAN_out = D(x)
    GAN = Model(GAN_in, GAN_out)
    GAN.compile(loss=&#39;binary_crossentropy&#39;, optimizer=G.optimizer)
    return GAN, GAN_out

GAN_in = Input([10])
GAN, GAN_out = make_gan(GAN_in, G, D)
GAN.summary()</code></pre>
<hr />
<p>Layer (type) Output Shape Param #<br />
=================================================================
input_3 (InputLayer) (None, 10) 0<br />
_________________________________________________________________
model_1 (Model) (None, 30) 8230<br />
_________________________________________________________________
model_2 (Model) (None, 2) 23672<br />
=================================================================
Total params: 31,902
Trainable params: 8,230
Non-trainable params: 23,672
_________________________________________________________________</p>
<pre class="python"><code>def sample_data_and_gen(G, noise_dim=10, n_samples= num):
    XT = np.array(g_data)
    XN_noise = np.random.uniform(0, 1, size=[n_samples, noise_dim])
    XN = G.predict(XN_noise)
    X = np.concatenate((XT, XN))
    y = np.zeros((2*n_samples, 2))
    y[:n_samples, 1] = 1
    y[n_samples:, 0] = 1
    return X, y

def pretrain(G, D, noise_dim=10, n_samples = num, batch_size=32):
    X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)
    set_trainability(D, True)
    D.fit(X, y, epochs=1, batch_size=batch_size)
    
pretrain(G, D)

def sample_noise(G, noise_dim=10, n_samples=num):
    X = np.random.uniform(0, 1, size=[n_samples, noise_dim])
    y = np.zeros((n_samples, 2))
    y[:, 1] = 1
    return X, y</code></pre>
<p>Epoch 1/1
492/492 [==============================] - ETA: 7s - loss: 0.662 - 1s 1ms/step - loss: 0.4410</p>
</div>
<div id="section-2" class="section level2">
<h2>訓練生成對抗網路</h2>
<p>訓練生成對抗網路，並存取最後一次生成器產生結果</p>
<pre class="python"><code>def train(GAN, G, D, epochs=100, n_samples= num, noise_dim=10, batch_size=32, verbose=False, v_freq=dim,):
    d_loss = []
    g_loss = []
    e_range = range(epochs)
    if verbose:
        e_range = tqdm(e_range)
    for epoch in e_range:
        X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)
        set_trainability(D, True)
        d_loss.append(D.train_on_batch(X, y))
        xx,yy = X,y
        
        X, y = sample_noise(G, n_samples=n_samples, noise_dim=noise_dim)
        set_trainability(D, False)
        g_loss.append(GAN.train_on_batch(X, y))
        if verbose and (epoch + 1) % v_freq == 0:
            print(&quot;Epoch #{}: Generative Loss: {}, Discriminative Loss: {}&quot;.format(epoch + 1, g_loss[-1], d_loss[-1]))
    return d_loss, g_loss, xx, yy

d_loss, g_loss ,xx,yy= train(GAN, G, D, verbose=True)</code></pre>
<p>HBox(children=(IntProgress(value=0), HTML(value=’’)))</p>
<p>Epoch #30: Generative Loss: 4.016930103302002, Discriminative Loss: 0.05247233808040619
Epoch #60: Generative Loss: 4.552446365356445, Discriminative Loss: 0.03850032016634941
Epoch #90: Generative Loss: 4.267617702484131, Discriminative Loss: 0.12488271296024323</p>
<p>自動化echo</p>
</div>
<div id="section-3" class="section level2">
<h2>損失函數</h2>
<pre class="python"><code>ax = pd.DataFrame(
    {
        &#39;Generative Loss&#39;: g_loss,
        &#39;Discriminative Loss&#39;: d_loss,
    }
).plot(title=&#39;Training loss&#39;, logy=True)
ax.set_xlabel(&quot;Epochs&quot;)
ax.set_ylabel(&quot;Loss&quot;)</code></pre>
<p>Text(0, 0.5, ‘Loss’)</p>
<div class="figure">
<img src="output_19_1.png" alt="png" />
<p class="caption">png</p>
</div>
<pre class="python"><code>new_data = xx[num:num*2+1]
#pd.DataFrame(new_data)</code></pre>
<pre class="python"><code>#pd.DataFrame(ss.inverse_transform(xx[0:492]))
#pd.DataFrame(ss.inverse_transform(xx))</code></pre>
</div>
</div>
<div id="one-class-learning" class="section level1">
<h1>One Class Learning</h1>
<div id="one-class-svm" class="section level2">
<h2>one class svm</h2>
<pre class="python"><code>from sklearn import svm

clf = svm.OneClassSVM(kernel=&#39;rbf&#39;, gamma=&#39;auto&#39;).fit(xx[0:246])

origin = pd.DataFrame(clf.score_samples(xx[0:246]))
origin.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
246.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
23.916780
</td>
</tr>
<tr>
<th>
std
</th>
<td>
6.798673
</td>
</tr>
<tr>
<th>
min
</th>
<td>
1.089954
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
20.998108
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
25.390590
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
28.886752
</td>
</tr>
<tr>
<th>
max
</th>
<td>
33.391387
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>new = pd.DataFrame(clf.score_samples(xx[246:493]))
new.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
0
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
246.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
29.028060
</td>
</tr>
<tr>
<th>
std
</th>
<td>
1.533237
</td>
</tr>
<tr>
<th>
min
</th>
<td>
25.520972
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
27.996268
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
28.910111
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
30.022437
</td>
</tr>
<tr>
<th>
max
</th>
<td>
33.144183
</td>
</tr>
</tbody>
</table>
</div>
<pre class="python"><code>occ = pd.concat([pd.DataFrame(new[0] &lt; origin[0].min()),pd.DataFrame(new[0] &gt; origin[0].max())], axis=1)
occ[&#39;ava&#39;] = pd.DataFrame(occ.iloc[:,1:2] == occ.iloc[:,0:1])
occ</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
0
</th>
<th>
0
</th>
<th>
ava
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
1
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
2
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
3
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
4
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
5
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
6
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
7
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
8
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
9
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
10
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
11
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
12
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
13
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
14
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
15
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
16
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
17
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
18
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
19
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
20
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
21
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
22
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
23
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
24
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
25
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
26
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
27
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
28
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
29
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
216
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
217
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
218
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
219
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
220
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
221
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
222
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
223
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
224
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
225
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
226
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
227
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
228
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
229
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
230
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
231
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
232
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
233
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
234
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
235
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
236
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
237
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
238
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
239
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
240
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
241
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
242
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
243
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
244
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
245
</th>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
</tbody>
</table>
<p>
246 rows × 3 columns
</p>
</div>
<pre class="python"><code>err = sum(occ[&#39;ava&#39;] == False)/len(occ[&#39;ava&#39;])
err</code></pre>
<p>0.0</p>
</div>
</div>
<div id="section-4" class="section level1">
<h1>合併資料</h1>
<pre class="python"><code>re = 577
new_data = pd.DataFrame(xx[246:493])
new_data.columns = g_data.columns
data = pd.concat([g_data,new_data], axis=0)
for i in range(re):
    d_loss, g_loss ,x1,yy= train(GAN, G, D, epochs=1,verbose=True)
    new_data = pd.DataFrame(ss.inverse_transform(xx[246:493]))
    data = pd.concat([data,new_data], axis=0)
    
data[&#39;sum&#39;] = 1</code></pre>
<p>HBox(children=(IntProgress(value=0, max=1), HTML(value=’’)))</p>
</div>
<div id="section-5" class="section level1">
<h1>資料分析</h1>
<p>合併新舊資料</p>
<pre class="python"><code>data.columns = df.columns
data = pd.concat([data_train,data], axis=0)</code></pre>
<p>建構簡易分類器</p>
<pre class="python"><code>from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn import metrics

train_X = data.iloc[:,0:30]
test_X = data_test.iloc[:,0:30]
train_y = data[&quot;Class&quot;]
test_y = data_test[&quot;Class&quot;]


forest = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = forest.fit(train_X, train_y)

test_y_predicted = forest.predict(test_X)
accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)</code></pre>
<p>ACC = 0.999557596696722<br />
AUC = 0.9003572630796582</p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>將各採樣方法的結果建立表格並畫出</p>
<pre class="r"><code>result &lt;- data.frame()
result = data.frame(c(0.9994803480263759,0.8678545237199384))
result$Under_Sampling = data.frame(c(0.9995084373222475,0.8901876266312907))
result$SMOTE = data.frame(c(0.9994663033784401,0.9023405417267101))
result$ADASYN = data.frame(c(0.9993890578147933,0.8982438459344533))
result$GAN = data.frame(c(0.999557596696722,0.9003572630796582))
colnames(result) &lt;- c(&quot;Non Sampling&quot;,&quot;Under Sampling&quot;,&quot;SMOTE&quot;,&quot;ADASYN&quot;,&quot;GAN&quot;)
rownames(result) &lt;- c(&quot;ACC&quot;,&quot;AUC&quot;)
#result$index &lt;- c(&quot;ACC&quot;,&quot;AUC&quot;)
result  = data.frame(t(result))
result</code></pre>
<pre><code>##                      ACC       AUC
## Non Sampling   0.9994803 0.8678545
## Under Sampling 0.9995084 0.8901876
## SMOTE          0.9994663 0.9023405
## ADASYN         0.9993891 0.8982438
## GAN            0.9995576 0.9003573</code></pre>
<pre class="r"><code>library(ggplot2)
ggplot(data = result)+
  geom_point(mapping = aes(x = ACC,y = AUC,color = rownames(result)),size=7)</code></pre>
<p><img src="/post/2019-11-19-gan-sampling-versus-other-sampling-method-on-credit-card-fraud-detection-data_files/figure-html/unnamed-chunk-2-1.png" width="672" />
從上表可看出，若單考量AUC的情況下，SMOTE有較佳的結果、GAN則排在第二，若是以綜合考量的情況下(落於右上角的)，則是GAN的結果較佳，從此表可看出，SMOTE與我的One class-GAN在AUC有差不多的結果，而單就運算成本來說GAN則遠遠高出不少，但在資料不大的情況下(像此例)，則可以交叉多種採樣方法來使用，後續可能會使用其他種方法或是資料來比較GAN與其他採樣方法的效果，畢竟還是希望GAN在資料過採樣上能有較好的表現。</p>
</div>

</main>


















<nav class="post-nav">
  <span class="nav-prev"><a href="/post/2019/11/25/gan-based-small-sample-augmentation/">&larr; GAN Based Small Sample Augmentation</a></span>
  <span class="nav-next"><a href="/post/2019/11/14/practice-in-gan-with-one-class-learning/">Practice in GAN with One Class Learning &rarr;</a></span>
</nav>



</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>

<footer>

<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
    <li><a href="/tags/"><span data-hover="Tags">Tags</span></a></li>
    
    <li><a href="/about/"><span data-hover="Blogdown">Blogdown</span></a></li>
    
  </ul>
  
  <div class="copyright">&copy; <a href="/about1/">Lin</a> | <a href="https://github.com/NSYSUHermit">Github</a> | <a href="https://rpubs.com/JupiterHenry">Rpubs</a></div>
  
</div>
</footer>


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>




</body>
</html>

