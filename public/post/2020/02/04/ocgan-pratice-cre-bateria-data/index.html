<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>OCGAN Pratice: CRE Bateria data | Lin&#39;s Blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">

  </head>

  <body class="page">
    <nav class="header">
      <div class="banner">
<a href="/" class="text">
&Lfr;&Ifr;&Nfr;'&Sfr; &Bfr;&Lfr;&Ofr;&Gfr;
</a>
</div>

      <div class="head-meta">
      
        <span><a href="/">&larr; Back to Home</a></span>
        <span class="date">2020-02-04</span>
        
        
        
          
        
        
        
        <span><a href="https://github.com/yihui/hugo-xmag/edit/master/exampleSite/content/post/2020-02-04-ocgan-pratice-cre-bateria-data.Rmd">Edit this page &rarr;</a></span>
        
        
      
      </div>
    </nav>

<div class="container">
<article>
<div class="article-meta">

  <div class="categories">
  
    <a href="/categories/gan">gan</a>
  
     &hercon; <a href="/categories/deep-learning">deep-learning</a>
  
     &hercon; <a href="/categories/python">Python</a>
  
  </div>

  <h1><span class="title">OCGAN Pratice: CRE Bateria data</span></h1>

  
  <h3 class="author">Hermit
</h3>
  

  
  <p>Tags: <a href="/tags/classification">classification</a>; <a href="/tags/neural-network">neural network</a>
  </p>
  
  

</div>



<main>



<p>這次使用之前分析過的CRE資料，來嘗試使用OCGAN，但因原先資料CRE:NON比數為46:49，為了達到不平衡的效果，因此最後採用16:49的比例，從46個CRE中取隨機16個，而資料的訓練集以及驗證集比例為下：<br />
Train Set(CRE:Non): 6:19<br />
Validation Set(CRE:Non): 10:30</p>
<p>接著我們將分為有使用OCGAN平衡數據集的資料以及未平衡數據集的資料進行分別建模，兩者皆使用SVM的方式建模，並且先透過LOOCV的方式在數據集Tuning模型，最後套入驗證集計算總準確率、f1 score、auc等，比較有無平衡的效果。</p>
<div id="section" class="section level1">
<h1>讀入資料</h1>
<pre class="python"><code>import pandas as pd
import numpy as np

df = pd.read_csv(&#39;C:/Users/User/OneDrive - student.nsysu.edu.tw/Educations/NSYSU/fu_chung/bacterial/123.csv&#39;)</code></pre>
<pre class="python"><code>%matplotlib inline
by_fraud = df.groupby(&#39;CRE&#39;)
by_fraud.size().plot(kind = &#39;bar&#39;)</code></pre>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x238556a6dd8&gt;</code></pre>
<p><img src="/post/2020-02-04-ocgan-pratice-cre-bateria-data_files/output_1_1.png" /></p>
</div>
<div id="train-vs-test-set" class="section level1">
<h1>Train vs Test set</h1>
<p>train set (0:1):20:6<br />
test set(0:1):2.9:10</p>
<pre class="python"><code>from sklearn.model_selection import train_test_split
import random
cre = df[df[&#39;CRE&#39;].isin([1])].iloc[0:16,:]
cre[&#39;CRE&#39;] = 1
normal = df[df[&#39;CRE&#39;].isin([0])].iloc[:,:]
normal[&#39;CRE&#39;] = 0

random.seed(3)
train_nor, test_nor = train_test_split(normal, test_size = 0.6)
train_cre, test_cre = train_test_split(cre, test_size = 0.6)
data_train = pd.concat([train_nor,train_cre], axis=0)
data_test = pd.concat([test_nor,test_cre], axis=0) </code></pre>
<pre><code>C:\Users\User\Anaconda3\envs\Tensorflow-gpu\lib\site-packages\ipykernel_launcher.py:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  </code></pre>
</div>
<div id="variable-selection" class="section level1">
<h1>Variable Selection</h1>
<p>這次使用Best Subsets feature selection ，因為變數大約1400個，因此運算量極大，最終挑選169個變數，並依此建立後續模型。</p>
<pre class="python"><code>from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
fix_data = pd.DataFrame(sel.fit_transform(data_train))
fix_data</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
0
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
…
</th>
<th>
1112
</th>
<th>
1113
</th>
<th>
1114
</th>
<th>
1115
</th>
<th>
1116
</th>
<th>
1117
</th>
<th>
1118
</th>
<th>
1119
</th>
<th>
1120
</th>
<th>
1121
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.00
</td>
<td>
1373136.00
</td>
<td>
55669.65
</td>
<td>
29413.53
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1849143.63
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0.00
</td>
<td>
69101.06
</td>
<td>
3312204.50
</td>
<td>
43936.20
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.00
</td>
<td>
55406.89
</td>
<td>
37459.26
</td>
<td>
567906.94
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
574.92
</td>
<td>
2521.91
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.00
</td>
<td>
2209497.50
</td>
<td>
0.00
</td>
<td>
332850.19
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
88801.86
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
386523.69
</td>
<td>
19751.14
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
474353.41
</td>
<td>
…
</td>
<td>
98261.75
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
445534.22
</td>
<td>
57997.06
</td>
<td>
123774.34
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
6
</th>
<td>
0.00
</td>
<td>
410000.25
</td>
<td>
0.00
</td>
<td>
3239424.25
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
22582.54
</td>
<td>
275360.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
8
</th>
<td>
0.00
</td>
<td>
1656135.25
</td>
<td>
578440.81
</td>
<td>
444822.34
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
835118.25
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
9
</th>
<td>
0.00
</td>
<td>
7308348.00
</td>
<td>
0.00
</td>
<td>
428864.38
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
267987.5
</td>
<td>
25078.88
</td>
<td>
143299.28
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0.00
</td>
<td>
6349.39
</td>
<td>
44542.15
</td>
<td>
12410.13
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
509739.94
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
11
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
8855.12
</td>
<td>
617930.44
</td>
<td>
0.00
</td>
<td>
4434.14
</td>
<td>
535930.94
</td>
<td>
355806.91
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
12
</th>
<td>
0.00
</td>
<td>
77937.30
</td>
<td>
42382.20
</td>
<td>
5088.32
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
13
</th>
<td>
0.00
</td>
<td>
510357.69
</td>
<td>
8396.52
</td>
<td>
76988.38
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
81731.96
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
14
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1980458.38
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
15
</th>
<td>
0.00
</td>
<td>
43712.79
</td>
<td>
0.00
</td>
<td>
9753.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
16
</th>
<td>
0.00
</td>
<td>
13922.24
</td>
<td>
0.00
</td>
<td>
19162.79
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
4851091.5
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1199.36
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
17
</th>
<td>
0.00
</td>
<td>
7889.80
</td>
<td>
215028.61
</td>
<td>
10020.09
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
18
</th>
<td>
0.00
</td>
<td>
1614557.63
</td>
<td>
234533.52
</td>
<td>
496465.50
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
19
</th>
<td>
0.00
</td>
<td>
237456.30
</td>
<td>
489450.28
</td>
<td>
317787.66
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
20
</th>
<td>
850345.94
</td>
<td>
0.00
</td>
<td>
1142041.75
</td>
<td>
0.00
</td>
<td>
22913.52
</td>
<td>
83391.35
</td>
<td>
132192.95
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
21
</th>
<td>
0.00
</td>
<td>
60298.57
</td>
<td>
242256.88
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
28529.95
</td>
<td>
0.00
</td>
<td>
577876.63
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
22
</th>
<td>
404748.19
</td>
<td>
97165.77
</td>
<td>
800137.44
</td>
<td>
134355.13
</td>
<td>
0.00
</td>
<td>
285973.69
</td>
<td>
152992.25
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
23
</th>
<td>
458382.13
</td>
<td>
136389.20
</td>
<td>
412460.16
</td>
<td>
294669.03
</td>
<td>
46850.84
</td>
<td>
470360.91
</td>
<td>
70784.13
</td>
<td>
52197.92
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
3268253.75
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
24
</th>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
39572.38
</td>
<td>
102085.48
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
37461.19
</td>
<td>
0.00
</td>
<td>
…
</td>
<td>
0.00
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
0.00
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
<p>
25 rows × 1122 columns
</p>
</div>
<pre class="python"><code>from sklearn.datasets import load_boston
import pandas as pd
import numpy as np
import statsmodels.api as sm

def stepwise_selection(X, y, 
                       initial_list=[], 
                       threshold_in=0.01, 
                       threshold_out = 0.05, 
                       verbose=True):
    &quot;&quot;&quot; Perform a forward-backward feature selection 
    based on p-value from statsmodels.api.OLS
    Arguments:
        X - pandas.DataFrame with candidate features
        y - list-like with the target
        initial_list - list of features to start with (column names of X)
        threshold_in - include a feature if its p-value &lt; threshold_in
        threshold_out - exclude a feature if its p-value &gt; threshold_out
        verbose - whether to print the sequence of inclusions and exclusions
    Returns: list of selected features 
    Always set threshold_in &lt; threshold_out to avoid infinite looping.
    See https://en.wikipedia.org/wiki/Stepwise_regression for the details
    &quot;&quot;&quot;
    included = list(initial_list)
    while True:
        changed=False
        # forward step
        excluded = list(set(X.columns)-set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()
            new_pval[new_column] = model.pvalues[new_column]
        best_pval = new_pval.min()
        if best_pval &lt; threshold_in:
            best_feature = new_pval.argmin()
            included.append(best_feature)
            changed=True
            if verbose:
                print(&#39;Add  {:30} with p-value {:.6}&#39;.format(best_feature, best_pval))

        # backward step
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        # use all coefs except intercept
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max() # null if pvalues is empty
        if worst_pval &gt; threshold_out:
            changed=True
            worst_feature = pvalues.argmax()
            included.remove(worst_feature)
            if verbose:
                print(&#39;Drop {:30} with p-value {:.6}&#39;.format(worst_feature, worst_pval))
        if not changed:
            break
    return included</code></pre>
<pre class="python"><code>X = data_train.iloc[:,0:1471]
y = data_train.iloc[:,1471:1472]
result = stepwise_selection(X, y)

print(&#39;resulting features:&#39;)
result</code></pre>
<p>[‘V993’,
‘V322’,
‘V864’,
‘V689’,
‘V598’,
‘V1156’,
‘V240’,
‘V395’,
‘V1255’,
‘V1218’,
‘V634’,
‘V529’,
‘V869’,
‘V410’,
‘V521’,
‘V32’,
‘V1201’,
‘V478’,
‘V306’,
‘V964’,
‘V1122’,
‘V485’,
‘V690’,
‘V947’,
‘V677’,
‘V1444’,
‘V832’,
‘V1’,
‘V517’,
‘V351’,
‘V9’,
‘V109’,
‘V872’,
‘V518’,
‘V1239’,
‘V270’,
‘V695’,
‘V147’,
‘V524’,
‘V679’,
‘V320’,
‘V356’,
‘V232’,
‘V687’,
‘V112’,
‘V983’,
‘V146’,
‘V345’,
‘V520’,
‘V198’,
‘V59’,
‘V408’,
‘V110’,
‘V250’,
‘V1275’,
‘V60’,
‘V1253’,
‘V459’,
‘V522’,
‘V889’,
‘V403’,
‘V269’,
‘V87’,
‘V530’,
‘V839’,
‘V399’,
‘V861’,
‘V242’,
‘V823’,
‘V58’,
‘V627’,
‘V84’,
‘V321’,
‘V50’,
‘V483’,
‘V475’,
‘V1396’,
‘V1411’,
‘V1285’,
‘V1093’,
‘V1378’,
‘V413’,
‘V525’,
‘V671’,
‘V30’,
‘V95’,
‘V1199’,
‘V767’,
‘V809’,
‘V1404’,
‘V1401’,
‘V113’,
‘V1198’,
‘V1405’,
‘V1398’,
‘V1209’,
‘V1407’,
‘V1352’,
‘V271’,
‘V528’,
‘V805’,
‘V1397’,
‘V753’,
‘V200’,
‘V1400’,
‘V1408’,
‘V1394’,
‘V593’,
‘V1157’,
‘V233’,
‘V268’,
‘V576’,
‘V181’,
‘V1395’,
‘V820’,
‘V1257’,
‘V514’,
‘V669’,
‘V943’,
‘V489’,
‘V937’,
‘V486’,
‘V513’,
‘V1143’,
‘V966’,
‘V980’,
‘V1274’,
‘V1403’,
‘V343’,
‘V686’,
‘V653’,
‘V1281’,
‘V234’,
‘V1279’,
‘V523’,
‘V870’,
‘V959’,
‘V1278’,
‘V871’,
‘V5’,
‘V775’,
‘V845’,
‘V1211’,
‘V1110’,
‘V1273’,
‘V995’,
‘V1276’,
‘V873’,
‘V595’,
‘V1280’,
‘V1034’,
‘V1228’,
‘V1012’,
‘V1226’,
‘V1094’,
‘V511’,
‘V944’,
‘V1068’,
‘V1146’,
‘V313’,
‘V821’,
‘V122’,
‘V1227’,
‘V386’,
‘V771’,
‘V551’,
‘V538’,
‘V1220’,
‘V1179’]</p>
<pre class="python"><code>result = [&#39;V993&#39;, &#39;V322&#39;, &#39;V864&#39;, &#39;V689&#39;, &#39;V598&#39;, &#39;V1156&#39;, &#39;V240&#39;, &#39;V395&#39;, &#39;V1255&#39;, &#39;V1218&#39;, &#39;V634&#39;, &#39;V529&#39;, &#39;V869&#39;, &#39;V410&#39;, &#39;V521&#39;, &#39;V32&#39;, &#39;V1201&#39;, &#39;V478&#39;, &#39;V306&#39;, &#39;V964&#39;, &#39;V1122&#39;, &#39;V485&#39;, &#39;V690&#39;, &#39;V947&#39;, &#39;V677&#39;, &#39;V1444&#39;, &#39;V832&#39;, &#39;V1&#39;, &#39;V517&#39;, &#39;V351&#39;, &#39;V9&#39;, &#39;V109&#39;, &#39;V872&#39;, &#39;V518&#39;, &#39;V1239&#39;, &#39;V270&#39;, &#39;V695&#39;, &#39;V147&#39;, &#39;V524&#39;, &#39;V679&#39;, &#39;V320&#39;, &#39;V356&#39;, &#39;V232&#39;, &#39;V687&#39;, &#39;V112&#39;, &#39;V983&#39;, &#39;V146&#39;, &#39;V345&#39;, &#39;V520&#39;, &#39;V198&#39;, &#39;V59&#39;, &#39;V408&#39;, &#39;V110&#39;, &#39;V250&#39;, &#39;V1275&#39;, &#39;V60&#39;, &#39;V1253&#39;, &#39;V459&#39;, &#39;V522&#39;, &#39;V889&#39;, &#39;V403&#39;, &#39;V269&#39;, &#39;V87&#39;, &#39;V530&#39;, &#39;V839&#39;, &#39;V399&#39;, &#39;V861&#39;, &#39;V242&#39;, &#39;V823&#39;, &#39;V58&#39;, &#39;V627&#39;, &#39;V84&#39;, &#39;V321&#39;, &#39;V50&#39;, &#39;V483&#39;, &#39;V475&#39;, &#39;V1396&#39;, &#39;V1411&#39;, &#39;V1285&#39;, &#39;V1093&#39;, &#39;V1378&#39;, &#39;V413&#39;, &#39;V525&#39;, &#39;V671&#39;, &#39;V30&#39;, &#39;V95&#39;, &#39;V1199&#39;, &#39;V767&#39;, &#39;V809&#39;, &#39;V1404&#39;, &#39;V1401&#39;, &#39;V113&#39;, &#39;V1198&#39;, &#39;V1405&#39;, &#39;V1398&#39;, &#39;V1209&#39;, &#39;V1407&#39;, &#39;V1352&#39;, &#39;V271&#39;, &#39;V528&#39;, &#39;V805&#39;, &#39;V1397&#39;, &#39;V753&#39;, &#39;V200&#39;, &#39;V1400&#39;, &#39;V1408&#39;, &#39;V1394&#39;, &#39;V593&#39;, &#39;V1157&#39;, &#39;V233&#39;, &#39;V268&#39;, &#39;V576&#39;, &#39;V181&#39;, &#39;V1395&#39;, &#39;V820&#39;, &#39;V1257&#39;, &#39;V514&#39;, &#39;V669&#39;, &#39;V943&#39;, &#39;V489&#39;, &#39;V937&#39;, &#39;V486&#39;, &#39;V513&#39;, &#39;V1143&#39;, &#39;V966&#39;, &#39;V980&#39;, &#39;V1274&#39;, &#39;V1403&#39;, &#39;V343&#39;, &#39;V686&#39;, &#39;V653&#39;, &#39;V1281&#39;, &#39;V234&#39;, &#39;V1279&#39;, &#39;V523&#39;, &#39;V870&#39;, &#39;V959&#39;, &#39;V1278&#39;, &#39;V871&#39;, &#39;V5&#39;, &#39;V775&#39;, &#39;V845&#39;, &#39;V1211&#39;, &#39;V1110&#39;, &#39;V1273&#39;, &#39;V995&#39;, &#39;V1276&#39;, &#39;V873&#39;, &#39;V595&#39;, &#39;V1280&#39;, &#39;V1034&#39;, &#39;V1228&#39;, &#39;V1012&#39;, &#39;V1226&#39;, &#39;V1094&#39;, &#39;V511&#39;, &#39;V944&#39;, &#39;V1068&#39;, &#39;V1146&#39;, &#39;V313&#39;, &#39;V821&#39;, &#39;V122&#39;, &#39;V1227&#39;, &#39;V386&#39;, &#39;V771&#39;, &#39;V551&#39;, &#39;V538&#39;, &#39;V1220&#39;, &#39;V1179&#39;]</code></pre>
<pre class="python"><code>train = data_train.loc[:,result]
test = data_test.loc[:,result]

train_X = new_data_train.iloc[:,:]
test_X = data_test.iloc[:,:]
train_y = new_data_train[&quot;CRE&quot;]
test_y = data_test[&quot;CRE&quot;]

d_nor = train_nor.loc[:,result]
d_cre = train_cre.loc[:,result]
d_test = pd.concat([test_nor.loc[:,result],test_cre.loc[:,result]], axis=0)</code></pre>
</div>
<div id="non-sampling" class="section level1">
<h1>non sampling</h1>
<p>loocv</p>
<pre class="python"><code>def loocv(ldf):
    ldf = ldf.reset_index(drop=True)
    cv = []
    for i in range(len(ldf)):
        dtrain = ldf.drop([i])
        dtest = ldf.iloc[i:i+1,:]
        train_X = dtrain.iloc[:,0:ldf.shape[1]-1]
        test_X = dtest.iloc[:,0:ldf.shape[1]-1]
        train_y = dtrain[&quot;CRE&quot;]
        test_y = dtest[&quot;CRE&quot;]
        clf = svm.SVC(kernel = &#39;linear&#39;) #SVM模組，svc,線性核函式 
        clf_fit = clf.fit(train_X, train_y)
        test_y_predicted = clf.predict(test_X)
        accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
        cv += [accuracy_rf]
    loocv = np.mean(cv)
    return loocv</code></pre>
<pre class="python"><code>df1 = train
df1[&quot;CRE&quot;] = data_train[&quot;CRE&quot;]
loocv(df1)</code></pre>
<p>0.96</p>
<pre class="python"><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn import metrics
from sklearn import svm 

train_X = train
test_X = test
train_y = data_train[&quot;CRE&quot;]
test_y = data_test[&quot;CRE&quot;]


#forest = ensemble.RandomForestClassifier(n_estimators = 10)
#forest_fit = forest.fit(train_X, train_y)
clf = svm.SVC(kernel = &#39;linear&#39;) #SVM模組，svc,線性核函式 
clf_fit = clf.fit(train_X, train_y)
test_y_predicted = clf.predict(test_X)

accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)

import sklearn
f1 = sklearn.metrics.f1_score(test_y, test_y_predicted)
print(f1)</code></pre>
<p>1.0<br />
1.0<br />
1.0</p>
<div id="ocgan-for-balance-data" class="section level2">
<h2>OCGAN for balance data</h2>
<pre class="python"><code>### import modules
%matplotlib inline
import os
import random
import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm_notebook as tqdm
from keras.models import Model
from keras.layers import Input, Reshape
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import UpSampling1D, Conv1D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, SGD,RMSprop
from keras.callbacks import TensorBoard
from sklearn.preprocessing import StandardScaler

# set parameters
dim = 169
num = 6
g_data = d_cre 

# Standard Scaler
ss = StandardScaler()
g_data = pd.DataFrame(ss.fit_transform(g_data))

# wasserstein_loss
from keras import backend 
# implementation of wasserstein loss
def wasserstein_loss(y_true, y_pred):
    return backend.mean(y_true * y_pred)

# generator
def get_generative(G_in, dense_dim=200, out_dim= dim, lr=1e-3):
    x = Dense(dense_dim)(G_in)
    x = Activation(&#39;tanh&#39;)(x)
    G_out = Dense(out_dim, activation=&#39;tanh&#39;)(x)
    G = Model(G_in, G_out)
    opt = keras.optimizers.RMSprop(lr=lr)#原先為SGD
    G.compile(loss=wasserstein_loss, optimizer=opt)#原loss為binary_crossentropy
    return G, G_out

G_in = Input(shape=[10])
G, G_out = get_generative(G_in)
G.summary()

# discriminator
def get_discriminative(D_in, lr=1e-3, drate=.25, n_channels= dim, conv_sz=5, leak=.2):#lr=1e-3, drate=.25, n_channels= dim, conv_sz=5, leak=.2
    x = Reshape((-1, 1))(D_in)
    x = Conv1D(n_channels, conv_sz, activation=&#39;relu&#39;)(x)
    x = Dropout(drate)(x)
    x = Flatten()(x)
    x = Dense(n_channels)(x)
    D_out = Dense(2, activation=&#39;linear&#39;)(x)#sigmoid
    D = Model(D_in, D_out)
    dopt = keras.optimizers.RMSprop(lr=lr)#原先為Adam
    D.compile(loss=wasserstein_loss, optimizer=dopt)
    return D, D_out

D_in = Input(shape=[dim])
D, D_out = get_discriminative(D_in)
D.summary()

# set up gan
def set_trainability(model, trainable=False):
    model.trainable = trainable
    for layer in model.layers:
        layer.trainable = trainable
        
def make_gan(GAN_in, G, D):
    set_trainability(D, False)
    x = G(GAN_in)
    GAN_out = D(x)
    GAN = Model(GAN_in, GAN_out)
    GAN.compile(loss=wasserstein_loss, optimizer=G.optimizer)#元loss為binary_crossentropy
    return GAN, GAN_out

GAN_in = Input([10])
GAN, GAN_out = make_gan(GAN_in, G, D)
GAN.summary()

# pre train
def sample_data_and_gen(G, noise_dim=10, n_samples= num):
    XT = np.array(g_data)
    XN_noise = np.random.uniform(0, 1, size=[n_samples, noise_dim])
    XN = G.predict(XN_noise)
    X = np.concatenate((XT, XN))
    y = np.zeros((2*n_samples, 2))
    y[:n_samples, 1] = 1
    y[n_samples:, 0] = 1
    return X, y

def pretrain(G, D, noise_dim=10, n_samples = num, batch_size=32):
    X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)
    set_trainability(D, True)
    D.fit(X, y, epochs=1, batch_size=batch_size)
    
pretrain(G, D)

def sample_noise(G, noise_dim=10, n_samples=num):
    X = np.random.uniform(0, 1, size=[n_samples, noise_dim])
    y = np.zeros((n_samples, 2))
    y[:, 1] = 1
    return X, y

# one class detector
def oneclass(data,kernel = &#39;rbf&#39;,gamma = &#39;auto&#39;):
    num1 = int(len(data)/2)
    num2 = int(len(data)+1)
    from sklearn import svm
    clf = svm.OneClassSVM(kernel=kernel, gamma=gamma).fit(data[0:num1])
    origin = pd.DataFrame(clf.score_samples(data[0:num1]))
    new = pd.DataFrame(clf.score_samples(data[num1:num2]))

    occ = pd.concat([pd.DataFrame(new[0] &lt; origin[0].min()),pd.DataFrame(new[0] &gt; origin[0].max())], axis=1)
    occ[&#39;ava&#39;] = pd.DataFrame(occ.iloc[:,1:2] == occ.iloc[:,0:1])
    err = sum(occ[&#39;ava&#39;] == False)/len(occ[&#39;ava&#39;])
    return err

# productor
def gen(GAN, G, D, times=50, n_samples= num, noise_dim=10, batch_size=32, verbose=False, v_freq=dim,):
    data = pd.DataFrame()
    for epoch in range(times):
        X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)
        set_trainability(D, True)   
        xx,yy = X,y
        err = oneclass(xx)        
        num1 = int(len(xx)/2)
        num2 = int(len(xx)+1)
        
        data = pd.concat([data,pd.DataFrame(ss.inverse_transform(xx[num1:num2]))],axis = 0)              
        print(&quot;The %d times generator one class svm Error Rate=%f&quot; %(epoch, err))           
            
    return data

# training
def train(GAN, G, D, epochs=1, n_samples= num, noise_dim=10, batch_size=32, verbose=False, v_freq=dim,):
    d_loss = []
    g_loss = []
    e_range = range(epochs)
    if verbose:
        e_range = tqdm(e_range)
    for epoch in e_range:
        X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)
        set_trainability(D, True)
        d_loss.append(D.train_on_batch(X, y))
        xx,yy = X,y
        err = oneclass(xx) 
        print(&quot;The %d times epoch one class svm Error Rate=%f&quot; %(epoch, err))
        
        X, y = sample_noise(G, n_samples=n_samples, noise_dim=noise_dim)
        set_trainability(D, False)
        g_loss.append(GAN.train_on_batch(X, y))
        if verbose and (epoch + 1) % v_freq == 0:
            print(&quot;Epoch #{}: Generative Loss: {}, Discriminative Loss: {}&quot;.format(epoch + 1, g_loss[-1], d_loss[-1]))       
            
    return d_loss, g_loss, xx, yy

d_loss, g_loss ,xx,yy= train(GAN, G, D, verbose=True)</code></pre>
<hr />
<p>Layer (type) Output Shape Param #<br />
=================================================================
input_22 (InputLayer) (None, 10) 0<br />
_________________________________________________________________
dense_29 (Dense) (None, 200) 2200<br />
_________________________________________________________________
activation_8 (Activation) (None, 200) 0<br />
_________________________________________________________________
dense_30 (Dense) (None, 169) 33969<br />
=================================================================
Total params: 36,169
Trainable params: 36,169
Non-trainable params: 0
_________________________________________________________________
_________________________________________________________________
Layer (type) Output Shape Param #<br />
=================================================================
input_23 (InputLayer) (None, 169) 0<br />
_________________________________________________________________
reshape_8 (Reshape) (None, 169, 1) 0<br />
_________________________________________________________________
conv1d_8 (Conv1D) (None, 165, 169) 1014<br />
_________________________________________________________________
dropout_8 (Dropout) (None, 165, 169) 0<br />
_________________________________________________________________
flatten_8 (Flatten) (None, 27885) 0<br />
_________________________________________________________________
dense_31 (Dense) (None, 169) 4712734<br />
_________________________________________________________________
dense_32 (Dense) (None, 2) 340<br />
=================================================================
Total params: 4,714,088
Trainable params: 4,714,088
Non-trainable params: 0
_________________________________________________________________
_________________________________________________________________
Layer (type) Output Shape Param #<br />
=================================================================
input_24 (InputLayer) (None, 10) 0<br />
_________________________________________________________________
model_22 (Model) (None, 169) 36169<br />
_________________________________________________________________
model_23 (Model) (None, 2) 4714088<br />
=================================================================
Total params: 4,750,257
Trainable params: 36,169
Non-trainable params: 4,714,088
_________________________________________________________________
Epoch 1/1
12/12 [==============================] - 1s 48ms/step - loss: -0.0091</p>
<p>HBox(children=(IntProgress(value=0, max=1), HTML(value=’’)))</p>
<p>The 0 times epoch one class svm Error Rate=1.000000</p>
<pre class="python"><code>d_loss, g_loss ,xx,yy= train(GAN, G, D, epochs=300, verbose=True)</code></pre>
</div>
</div>
<div id="loss-plot" class="section level1">
<h1>loss plot</h1>
<pre class="python"><code>ax = pd.DataFrame(
    {
        &#39;Generative Loss&#39;: g_loss,
        &#39;Discriminative Loss&#39;: d_loss,
    }
).plot(title=&#39;Training loss&#39;, logy=False)
ax.set_xlabel(&quot;Epochs&quot;)
ax.set_ylabel(&quot;Loss&quot;)</code></pre>
<p>Text(0, 0.5, ‘Loss’)</p>
<p><img src="/post/2020-02-04-ocgan-pratice-cre-bateria-data_files/output_19_1.png" /></p>
</div>
<div id="generate-and-bulid-models" class="section level1">
<h1>generate and bulid models</h1>
<pre class="python"><code>new_data = gen(GAN, G, D, times = 2,verbose=True)
new_data.columns = d_nor.columns
new_data[&#39;CRE&#39;] = 1
d_train = pd.concat([d_nor,d_cre],axis = 0)
d_train[&quot;CRE&quot;] = data_train[&quot;CRE&quot;]
new_data_train = pd.concat([d_train,new_data],axis = 0)
new_data_train</code></pre>
<p>The 0 times generator one class svm Error Rate=1.000000
The 1 times generator one class svm Error Rate=1.000000</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
V993
</th>
<th>
V322
</th>
<th>
V864
</th>
<th>
V689
</th>
<th>
V598
</th>
<th>
V1156
</th>
<th>
V240
</th>
<th>
V395
</th>
<th>
V1255
</th>
<th>
V1218
</th>
<th>
…
</th>
<th>
V821
</th>
<th>
V122
</th>
<th>
V1227
</th>
<th>
V386
</th>
<th>
V771
</th>
<th>
V551
</th>
<th>
V538
</th>
<th>
V1220
</th>
<th>
V1179
</th>
<th>
CRE
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
84
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
1616.050000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
1524.790000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
576.870000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
59
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
93
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
49
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
89
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
46
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
532747.500000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
50
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
73
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
3.789579e+06
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
53
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
75932.230000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
90
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
120355.780000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
72
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
6.081896e+04
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
55
</th>
<td>
0.000000e+00
</td>
<td>
43165.340000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
1.610336e+07
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
80
</th>
<td>
2.226269e+05
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
58034.680000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
62
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
47390.040000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
107487.350000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
78
</th>
<td>
0.000000e+00
</td>
<td>
4350.400000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
47365.650000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
63
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
79
</th>
<td>
7.658691e+05
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
6783.660000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
385997.310000
</td>
<td>
0.000000
</td>
<td>
23789.800000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
17489.530000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
75
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
31978.320000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
88
</th>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
4.644874e+04
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0
</td>
</tr>
<tr>
<th>
12
</th>
<td>
4.961734e+06
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
145406.670000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
2.020681e+07
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
204892.950000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
4.932036e+06
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
72948.780000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
8
</th>
<td>
9.387160e+06
</td>
<td>
79444.560000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
5.207452e+04
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
6
</th>
<td>
6.053306e+06
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
13
</th>
<td>
1.272117e+07
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
…
</td>
<td>
0.000000e+00
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
1
</td>
</tr>
<tr>
<th>
0
</th>
<td>
1.512491e+07
</td>
<td>
-16218.833383
</td>
<td>
-0.993609
</td>
<td>
0.995566
</td>
<td>
-9.963942e-01
</td>
<td>
-0.996621
</td>
<td>
0.994142
</td>
<td>
0.998140
</td>
<td>
-0.995715
</td>
<td>
-0.996149
</td>
<td>
…
</td>
<td>
-1.062502e+04
</td>
<td>
-29740.948329
</td>
<td>
-0.998415
</td>
<td>
0.997263
</td>
<td>
0.994520
</td>
<td>
-0.997255
</td>
<td>
-0.993365
</td>
<td>
121635.468003
</td>
<td>
-0.993095
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1.506312e+07
</td>
<td>
-16018.118402
</td>
<td>
-0.985025
</td>
<td>
0.989016
</td>
<td>
-9.891991e-01
</td>
<td>
-0.990969
</td>
<td>
0.981192
</td>
<td>
0.993236
</td>
<td>
-0.987260
</td>
<td>
-0.986484
</td>
<td>
…
</td>
<td>
-1.056079e+04
</td>
<td>
-29401.546549
</td>
<td>
-0.991166
</td>
<td>
0.994364
</td>
<td>
0.985471
</td>
<td>
-0.986429
</td>
<td>
-0.985721
</td>
<td>
120628.955686
</td>
<td>
-0.984401
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1.511217e+07
</td>
<td>
-16203.647890
</td>
<td>
-0.992818
</td>
<td>
0.993807
</td>
<td>
-9.955698e-01
</td>
<td>
-0.995253
</td>
<td>
0.990759
</td>
<td>
0.996745
</td>
<td>
-0.993857
</td>
<td>
-0.993840
</td>
<td>
…
</td>
<td>
-1.061835e+04
</td>
<td>
-29667.786330
</td>
<td>
-0.996489
</td>
<td>
0.996588
</td>
<td>
0.992731
</td>
<td>
-0.993555
</td>
<td>
-0.990812
</td>
<td>
121295.633443
</td>
<td>
-0.992025
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1.496042e+07
</td>
<td>
-15581.279153
</td>
<td>
-0.966205
</td>
<td>
0.968444
</td>
<td>
-9.806563e-01
</td>
<td>
-0.977659
</td>
<td>
0.957520
</td>
<td>
0.980549
</td>
<td>
-0.969081
</td>
<td>
-0.972953
</td>
<td>
…
</td>
<td>
-1.025045e+04
</td>
<td>
-28607.187664
</td>
<td>
-0.987470
</td>
<td>
0.981976
</td>
<td>
0.964365
</td>
<td>
-0.969920
</td>
<td>
-0.963239
</td>
<td>
118564.882265
</td>
<td>
-0.971673
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1.513735e+07
</td>
<td>
-16285.360136
</td>
<td>
-0.997202
</td>
<td>
0.997497
</td>
<td>
-9.974456e-01
</td>
<td>
-0.997892
</td>
<td>
0.997145
</td>
<td>
0.998738
</td>
<td>
-0.998016
</td>
<td>
-0.997765
</td>
<td>
…
</td>
<td>
-1.067976e+04
</td>
<td>
-29855.686473
</td>
<td>
-0.999103
</td>
<td>
0.998467
</td>
<td>
0.996284
</td>
<td>
-0.997739
</td>
<td>
-0.997499
</td>
<td>
121800.363942
</td>
<td>
-0.996244
</td>
<td>
1
</td>
</tr>
<tr>
<th>
5
</th>
<td>
1.507087e+07
</td>
<td>
-15967.363031
</td>
<td>
-0.976027
</td>
<td>
0.983896
</td>
<td>
-9.862480e-01
</td>
<td>
-0.986026
</td>
<td>
0.980322
</td>
<td>
0.993358
</td>
<td>
-0.987903
</td>
<td>
-0.982607
</td>
<td>
…
</td>
<td>
-1.038747e+04
</td>
<td>
-29348.452340
</td>
<td>
-0.988892
</td>
<td>
0.990501
</td>
<td>
0.981182
</td>
<td>
-0.988225
</td>
<td>
-0.981214
</td>
<td>
120886.994780
</td>
<td>
-0.974020
</td>
<td>
1
</td>
</tr>
<tr>
<th>
0
</th>
<td>
1.508056e+07
</td>
<td>
-15925.283069
</td>
<td>
-0.987930
</td>
<td>
0.987971
</td>
<td>
-9.846137e-01
</td>
<td>
-0.990841
</td>
<td>
0.990818
</td>
<td>
0.993492
</td>
<td>
-0.989569
</td>
<td>
-0.989493
</td>
<td>
…
</td>
<td>
-1.049298e+04
</td>
<td>
-29527.114775
</td>
<td>
-0.993623
</td>
<td>
0.992465
</td>
<td>
0.983135
</td>
<td>
-0.987305
</td>
<td>
-0.993185
</td>
<td>
121117.700984
</td>
<td>
-0.977792
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1.514196e+07
</td>
<td>
-16296.326162
</td>
<td>
-0.996452
</td>
<td>
0.996435
</td>
<td>
-9.980265e-01
</td>
<td>
-0.998247
</td>
<td>
0.996632
</td>
<td>
0.998864
</td>
<td>
-0.996634
</td>
<td>
-0.997642
</td>
<td>
…
</td>
<td>
-1.066461e+04
</td>
<td>
-29840.008210
</td>
<td>
-0.998694
</td>
<td>
0.998259
</td>
<td>
0.995150
</td>
<td>
-0.998254
</td>
<td>
-0.996857
</td>
<td>
121828.771543
</td>
<td>
-0.995327
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1.512663e+07
</td>
<td>
-16255.855632
</td>
<td>
-0.994796
</td>
<td>
0.996507
</td>
<td>
-9.964558e-01
</td>
<td>
-0.997275
</td>
<td>
0.993639
</td>
<td>
0.998080
</td>
<td>
-0.995786
</td>
<td>
-0.996104
</td>
<td>
…
</td>
<td>
-1.067495e+04
</td>
<td>
-29788.474074
</td>
<td>
-0.997894
</td>
<td>
0.998463
</td>
<td>
0.995382
</td>
<td>
-0.994885
</td>
<td>
-0.995674
</td>
<td>
121512.002636
</td>
<td>
-0.994164
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1.512467e+07
</td>
<td>
-16237.220094
</td>
<td>
-0.993826
</td>
<td>
0.994421
</td>
<td>
-9.969582e-01
</td>
<td>
-0.996626
</td>
<td>
0.993722
</td>
<td>
0.997794
</td>
<td>
-0.995277
</td>
<td>
-0.995618
</td>
<td>
…
</td>
<td>
-1.063710e+04
</td>
<td>
-29764.659522
</td>
<td>
-0.998138
</td>
<td>
0.997284
</td>
<td>
0.993458
</td>
<td>
-0.995569
</td>
<td>
-0.994352
</td>
<td>
121550.172951
</td>
<td>
-0.993698
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1.514689e+07
</td>
<td>
-16320.480008
</td>
<td>
-0.997326
</td>
<td>
0.998062
</td>
<td>
-9.979978e-01
</td>
<td>
-0.998915
</td>
<td>
0.997715
</td>
<td>
0.999290
</td>
<td>
-0.997620
</td>
<td>
-0.998389
</td>
<td>
…
</td>
<td>
-1.069688e+04
</td>
<td>
-29884.836932
</td>
<td>
-0.999016
</td>
<td>
0.999236
</td>
<td>
0.997096
</td>
<td>
-0.998345
</td>
<td>
-0.998637
</td>
<td>
121884.855218
</td>
<td>
-0.995719
</td>
<td>
1
</td>
</tr>
<tr>
<th>
5
</th>
<td>
1.513991e+07
</td>
<td>
-16296.945582
</td>
<td>
-0.996303
</td>
<td>
0.997118
</td>
<td>
-9.973860e-01
</td>
<td>
-0.998098
</td>
<td>
0.996116
</td>
<td>
0.998822
</td>
<td>
-0.996933
</td>
<td>
-0.997412
</td>
<td>
…
</td>
<td>
-1.067624e+04
</td>
<td>
-29835.214938
</td>
<td>
-0.998451
</td>
<td>
0.998534
</td>
<td>
0.995529
</td>
<td>
-0.998190
</td>
<td>
-0.996817
</td>
<td>
121807.697267
</td>
<td>
-0.995231
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
<p>
37 rows × 170 columns
</p>
</div>
<pre class="python"><code>loocv(new_data_train)</code></pre>
<p>1.0</p>
<pre class="python"><code>from sklearn.model_selection import train_test_split
from sklearn import ensemble
from sklearn import metrics

train_X = new_data_train.iloc[:,0:169]
test_X = test.iloc[:,:]
train_y = new_data_train[&quot;CRE&quot;]
test_y = data_test[&quot;CRE&quot;]


forest = ensemble.RandomForestClassifier(n_estimators = 10)
forest_fit = forest.fit(train_X, train_y)

test_y_predicted = forest.predict(test_X)
accuracy_rf = metrics.accuracy_score(test_y, test_y_predicted)
print(accuracy_rf)


test_auc = metrics.roc_auc_score(test_y, test_y_predicted)
print (test_auc)

import sklearn
f1 = sklearn.metrics.f1_score(test_y, test_y_predicted)
print(f1)</code></pre>
<p>1.0<br />
1.0<br />
1.0</p>
</div>

</main>


















<nav class="post-nav">
  <span class="nav-prev"><a href="/post/2020/02/10/cre-data-features-selection/">&larr; CRE data features selection</a></span>
  <span class="nav-next"><a href="/post/2020/01/16/ocgan-tuning/">OCGAN Tuning &rarr;</a></span>
</nav>



</article>
</div>

<script async src="//yihui.name/js/center-img.js"></script>

<footer>

<div class="footer">
  <ul class="menu">
    
    <li><a href="/"><span data-hover="Home">Home</span></a></li>
    
    <li><a href="/categories/"><span data-hover="Categories">Categories</span></a></li>
    
    <li><a href="/tags/"><span data-hover="Tags">Tags</span></a></li>
    
    <li><a href="/about/"><span data-hover="Blogdown">Blogdown</span></a></li>
    
  </ul>
  
  <div class="copyright">&copy; <a href="/about1/">Lin</a> | <a href="https://github.com/NSYSUHermit">Github</a> | <a href="https://rpubs.com/JupiterHenry">Rpubs</a></div>
  
</div>
</footer>


<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>




</body>
</html>

